{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResizeModel(nn.Module):\n",
    "    def __init__(self, output=(224,224)):\n",
    "        super(ResizeModel, self).__init__()\n",
    "\n",
    "        self.output_size = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 调整输入张量的大小\n",
    "        x = F.interpolate(x, size=self.output_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "model = ResizeModel()\n",
    "\n",
    "# 创建一个输入张量，注意这里的尺寸需要是模型可以接受的尺寸\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 1520, 2688)\n",
    "\n",
    "# 导出模型为ONNX格式\n",
    "onnx_file = \"simple_resize_model.onnx\"\n",
    "torch.onnx.export(model,               # 模型要导出的对象\n",
    "                  dummy_input,          # 模型输入的dummy数据\n",
    "                  onnx_file,            # 输出的ONNX文件名\n",
    "                  export_params=True,   # 存储训练好的参数权重\n",
    "                  opset_version=11,     # ONNX操作符集版本\n",
    "                  do_constant_folding=True, # 执行常量折叠优化\n",
    "                  input_names = ['images'],   # 输入的名称\n",
    "                  output_names = ['output0'],# 输出的名称\n",
    "                #   dynamic_axes={'images' : {0 : 'batch_size'},    # 变量长度不定的维度名(Tensor的第二维开始)  \n",
    "                #                 'output0' : {0 : 'batch_size'}}\n",
    "                  )\n",
    "\n",
    "print(f\"Model exported to {onnx_file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnxruntime as ort  \n",
    "import numpy as np  \n",
    "  \n",
    "# 加载ONNX模型  \n",
    "sess = ort.InferenceSession('simple_resize_model.onnx')  \n",
    "  \n",
    "# 获取输入和输出的名称  \n",
    "input_name = sess.get_inputs()[0].name  \n",
    "output_name = sess.get_outputs()[0].name  \n",
    "  \n",
    "# 准备输入数据（与训练时相同）  \n",
    "dummy_input = np.random.rand(1, 3, 1520, 2688).astype(np.float32)  \n",
    "  \n",
    "# 进行推理  \n",
    "ort_inputs = {input_name: dummy_input}  \n",
    "ort_outputs = sess.run([output_name], ort_inputs)  \n",
    "  \n",
    "# 检查输出张量的尺寸  \n",
    "print(ort_outputs[0].shape)  # 应该输出 (1, 3, 224, 224)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.onnx  \n",
    "  \n",
    "# 自定义一个包含reshape的层（尽管这不是常见的做法）  \n",
    "class ReshapeLayer(nn.Module):  \n",
    "    def __init__(self, *shape):  \n",
    "        super(ReshapeLayer, self).__init__()  \n",
    "        self.shape = shape  \n",
    "  \n",
    "    def forward(self, x):  \n",
    "        return x.view(*self.shape)  \n",
    "  \n",
    "# 定义一个简单的网络模型  \n",
    "class SimpleNetwork(nn.Module):  \n",
    "    def __init__(self):  \n",
    "        super(SimpleNetwork, self).__init__()  \n",
    "        self.reshape_layer = ReshapeLayer(1, 3, 224, 224)  \n",
    "  \n",
    "    def forward(self, x):  \n",
    "        x = self.reshape_layer(x)  \n",
    "        # 在这里可以添加其他层，但为了简单起见，我们直接返回reshape后的Tensor  \n",
    "        return x  \n",
    "  \n",
    "# 实例化网络模型  \n",
    "net = SimpleNetwork()  \n",
    "  \n",
    "# 创建一个随机的输入Tensor，注意ONNX需要定义输入的形状  \n",
    "dummy_input = torch.randn(1, 3, 1520, 2688)  \n",
    "  \n",
    "# 定义一个输出文件的名称  \n",
    "onnx_file = 'simple_network.onnx'  \n",
    "  \n",
    "# 导出模型为ONNX格式  \n",
    "# 注意：我们需要确保模型在评估模式下（即不启用dropout、batchnorm的training模式等）  \n",
    "net.eval()  \n",
    "with torch.no_grad():  # 不需要计算梯度  \n",
    "    torch.onnx.export(net, dummy_input, onnx_file, verbose=True, input_names=['input_tensor'], output_names=['output_tensor'])  \n",
    "  \n",
    "print(f\"Model exported to {onnx_file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 3, 224, 224]' is invalid for input of size 12257280",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 38\u001B[0m\n\u001B[0;32m     36\u001B[0m net\u001B[38;5;241m.\u001B[39meval()  \n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():  \u001B[38;5;66;03m# 不需要计算梯度  \u001B[39;00m\n\u001B[1;32m---> 38\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdummy_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monnx_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_tensor\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutput_tensor\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel exported to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00monnx_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\onnx\\__init__.py:275\u001B[0m, in \u001B[0;36mexport\u001B[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;124;03mExport a model into ONNX format.  This exporter runs your model\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;124;03monce in order to get a trace of its execution to be exported;\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;124;03m        than ONNX.\u001B[39;00m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[1;32m--> 275\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    276\u001B[0m \u001B[43m                    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maten\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_raw_ir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    277\u001B[0m \u001B[43m                    \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_retain_param_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    279\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mstrip_doc_string\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    280\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_onnx_checker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_external_data_format\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\onnx\\utils.py:88\u001B[0m, in \u001B[0;36mexport\u001B[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001B[0m\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     87\u001B[0m         operator_export_type \u001B[38;5;241m=\u001B[39m OperatorExportTypes\u001B[38;5;241m.\u001B[39mONNX\n\u001B[1;32m---> 88\u001B[0m \u001B[43m_export\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_retain_param_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_retain_param_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexample_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexample_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrip_doc_string\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrip_doc_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_onnx_checker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_onnx_checker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_external_data_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_external_data_format\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\onnx\\utils.py:689\u001B[0m, in \u001B[0;36m_export\u001B[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference)\u001B[0m\n\u001B[0;32m    685\u001B[0m     dynamic_axes \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    686\u001B[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001B[0;32m    688\u001B[0m graph, params_dict, torch_out \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m--> 689\u001B[0m     \u001B[43m_model_to_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[43m                    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mexample_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_retain_param_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    692\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mval_do_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    693\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    694\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    695\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    697\u001B[0m \u001B[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001B[39;00m\n\u001B[0;32m    698\u001B[0m defer_weight_export \u001B[38;5;241m=\u001B[39m export_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ExportTypes\u001B[38;5;241m.\u001B[39mPROTOBUF_FILE\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\onnx\\utils.py:458\u001B[0m, in \u001B[0;36m_model_to_graph\u001B[1;34m(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(example_outputs, (torch\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mbool\u001B[39m)):\n\u001B[0;32m    456\u001B[0m     example_outputs \u001B[38;5;241m=\u001B[39m (example_outputs,)\n\u001B[1;32m--> 458\u001B[0m graph, params, torch_out, module \u001B[38;5;241m=\u001B[39m \u001B[43m_create_jit_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m                                                     \u001B[49m\u001B[43m_retain_param_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    461\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m _get_named_param_dict(graph, params)\n\u001B[0;32m    463\u001B[0m graph \u001B[38;5;241m=\u001B[39m _optimize_graph(graph, operator_export_type,\n\u001B[0;32m    464\u001B[0m                         _disable_torch_constant_prop\u001B[38;5;241m=\u001B[39m_disable_torch_constant_prop,\n\u001B[0;32m    465\u001B[0m                         fixed_batch_size\u001B[38;5;241m=\u001B[39mfixed_batch_size, params_dict\u001B[38;5;241m=\u001B[39mparams_dict,\n\u001B[0;32m    466\u001B[0m                         dynamic_axes\u001B[38;5;241m=\u001B[39mdynamic_axes, input_names\u001B[38;5;241m=\u001B[39minput_names,\n\u001B[0;32m    467\u001B[0m                         module\u001B[38;5;241m=\u001B[39mmodule)\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\onnx\\utils.py:422\u001B[0m, in \u001B[0;36m_create_jit_graph\u001B[1;34m(model, args, _retain_param_name)\u001B[0m\n\u001B[0;32m    420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m graph, params, torch_out, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 422\u001B[0m     graph, torch_out \u001B[38;5;241m=\u001B[39m \u001B[43m_trace_and_get_graph_from_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    423\u001B[0m     state_dict \u001B[38;5;241m=\u001B[39m _unique_state_dict(model)\n\u001B[0;32m    424\u001B[0m     params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(state_dict\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\onnx\\utils.py:373\u001B[0m, in \u001B[0;36m_trace_and_get_graph_from_model\u001B[1;34m(model, args)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_trace_and_get_graph_from_model\u001B[39m(model, args):\n\u001B[0;32m    367\u001B[0m \n\u001B[0;32m    368\u001B[0m     \u001B[38;5;66;03m# A basic sanity check: make sure the state_dict keys are the same\u001B[39;00m\n\u001B[0;32m    369\u001B[0m     \u001B[38;5;66;03m# before and after running the model.  Fail fast!\u001B[39;00m\n\u001B[0;32m    370\u001B[0m     orig_state_dict_keys \u001B[38;5;241m=\u001B[39m _unique_state_dict(model)\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[0;32m    372\u001B[0m     trace_graph, torch_out, inputs_states \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m--> 373\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_trace_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_force_outplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_return_inputs_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    374\u001B[0m     warn_on_static_input_change(inputs_states)\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m orig_state_dict_keys \u001B[38;5;241m!=\u001B[39m _unique_state_dict(model)\u001B[38;5;241m.\u001B[39mkeys():\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\jit\\_trace.py:1160\u001B[0m, in \u001B[0;36m_get_trace_graph\u001B[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001B[0m\n\u001B[0;32m   1158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(args, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m   1159\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args,)\n\u001B[1;32m-> 1160\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[43mONNXTracedModule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_force_outplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_return_inputs_states\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outs\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\jit\\_trace.py:127\u001B[0m, in \u001B[0;36mONNXTracedModule.forward\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    125\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(out_vars)\n\u001B[1;32m--> 127\u001B[0m graph, out \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_graph_by_tracing\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwrapper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43min_vars\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_create_interpreter_name_lookup_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_force_outplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return_inputs:\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m graph, outs[\u001B[38;5;241m0\u001B[39m], ret_inputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\jit\\_trace.py:118\u001B[0m, in \u001B[0;36mONNXTracedModule.forward.<locals>.wrapper\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return_inputs_states:\n\u001B[0;32m    117\u001B[0m     inputs_states\u001B[38;5;241m.\u001B[39mappend(_unflatten(in_args, in_desc))\n\u001B[1;32m--> 118\u001B[0m outs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtrace_inputs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return_inputs_states:\n\u001B[0;32m    120\u001B[0m     inputs_states[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m (inputs_states[\u001B[38;5;241m0\u001B[39m], trace_inputs)\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1039\u001B[0m, in \u001B[0;36mModule._slow_forward\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1037\u001B[0m         recording_scopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1039\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m recording_scopes:\n",
      "Cell \u001B[1;32mIn[4], line 21\u001B[0m, in \u001B[0;36mSimpleNetwork.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):  \n\u001B[1;32m---> 21\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# 在这里可以添加其他层，但为了简单起见，我们直接返回reshape后的Tensor  \u001B[39;00m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\Anaconda3\\envs\\NN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1039\u001B[0m, in \u001B[0;36mModule._slow_forward\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1037\u001B[0m         recording_scopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1039\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m recording_scopes:\n",
      "Cell \u001B[1;32mIn[4], line 12\u001B[0m, in \u001B[0;36mReshapeLayer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):  \n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[1, 3, 224, 224]' is invalid for input of size 12257280"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.onnx  \n",
    "  \n",
    "# 自定义一个包含reshape的层（尽管这不是常见的做法）  \n",
    "class ReshapeLayer(nn.Module):  \n",
    "    def __init__(self, *shape):  \n",
    "        super(ReshapeLayer, self).__init__()  \n",
    "        self.shape = shape  \n",
    "  \n",
    "    def forward(self, x):  \n",
    "        return x.view(*self.shape)  \n",
    "  \n",
    "# 定义一个简单的网络模型  \n",
    "class SimpleNetwork(nn.Module):  \n",
    "    def __init__(self):  \n",
    "        super(SimpleNetwork, self).__init__()  \n",
    "        self.reshape_layer = ReshapeLayer(1, 3, 224, 224)  \n",
    "  \n",
    "    def forward(self, x):  \n",
    "        x = self.reshape_layer(x)  \n",
    "        # 在这里可以添加其他层，但为了简单起见，我们直接返回reshape后的Tensor  \n",
    "        return x  \n",
    "  \n",
    "# 实例化网络模型  \n",
    "net = SimpleNetwork()  \n",
    "  \n",
    "# 创建一个随机的输入Tensor，注意ONNX需要定义输入的形状  \n",
    "dummy_input = torch.randn(1, 3, 1520, 2688)  \n",
    "  \n",
    "# 定义一个输出文件的名称  \n",
    "onnx_file = 'simple_network.onnx'  \n",
    "  \n",
    "# 导出模型为ONNX格式  \n",
    "# 注意：我们需要确保模型在评估模式下（即不启用dropout、batchnorm的training模式等）  \n",
    "net.eval()  \n",
    "with torch.no_grad():  # 不需要计算梯度  \n",
    "    torch.onnx.export(net, dummy_input, onnx_file, verbose=True, input_names=['input_tensor'], output_names=['output_tensor'])  \n",
    "  \n",
    "print(f\"Model exported to {onnx_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}